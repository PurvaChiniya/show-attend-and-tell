{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6652478552719295967\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3496374497123985203\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n",
      "length of training images 57579\n",
      "length of testing images 24678\n",
      "length of training labels 57579\n",
      "length of testing labels 24678\n",
      "(57579, 32, 250)\n",
      "(24678, 32, 250)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "from natsort import natsorted\n",
    "import os\n",
    "import numpy as np\n",
    "path=\"/home/tali/Desktop/PURVA_CHINIYA/IAMhand2printv5\"\n",
    "parameters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "global params\n",
    "params = {'path' : \"/home/tali/Desktop/PURVA_CHINIYA/IAMhand2printv5\",\n",
    "          'model_path': '/home/tali/Desktop/PURVA_CHINIYA',\n",
    "          'Batch_Size': 50,\n",
    "          'imgSize':(128,32),\n",
    "          'num_classes':len(parameters)+1,\n",
    "          'lr': 0.0001,\n",
    "          'num_epochs':1000,\n",
    "          'width_thresh':250}\n",
    "def get_file_paths(path=params['path']):\n",
    "    paths = np.array(natsorted([os.path.join(root, file) for root, dirs, files in os.walk(path) for file in files]))\n",
    "    WordList = np.array([os.path.basename(i).split('_')[-1][:-4] for i in paths])\n",
    "    index = np.random.permutation(len(paths))\n",
    "    paths = paths[index]\n",
    "    WordList = WordList[index]\n",
    "    return paths[:int(len(paths) * .7)], paths[int(len(paths) * .7):], WordList[:int(len(WordList) * .7)], WordList[int(\n",
    "        len(WordList) * .7):]\n",
    "def load_img(path):\n",
    "    img_ = cv2.imread(path)\n",
    "    [r, c] = img_.shape[:2]\n",
    "    r_ = 32\n",
    "    c_ = round(c/r * r_)\n",
    "    if c_ > 250:\n",
    "        c_ = 250\n",
    "    img_ = cv2.resize(img_, (c_,r_))  \n",
    "    \n",
    "    img_ = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
    "    img_ = cv2.copyMakeBorder(img_, 0, 0, 0, 250-c_, cv2.BORDER_CONSTANT, value=(255,255,255))\n",
    "    return img_/255,c_\n",
    "def Load_Batch(batch_paths):\n",
    "    batch_data = []\n",
    "    width=[]\n",
    "    for path in batch_paths:\n",
    "        img_,seq = load_img(path)\n",
    "        batch_data.append(img_)\n",
    "        width.append(seq)\n",
    "    return np.array(batch_data),np.array(seq)\n",
    "X_train,X_test,Y_train,Y_test=get_file_paths(path)\n",
    "\n",
    "print(\"length of training images\",len(X_train))\n",
    "print(\"length of testing images\",len(X_test))\n",
    "print(\"length of training labels\",len(Y_train))\n",
    "print(\"length of testing labels\",len(Y_test))\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "Test_array,Test_width=Load_Batch(X_test)\n",
    "Train_array,Train_width=Load_Batch(X_train)\n",
    "print(Train_array.shape)\n",
    "print(Test_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ResNetCNN():\n",
    "    \n",
    "    with tf.variable_scope(\"Network\"):\n",
    "        with tf.variable_scope(\"CNN\"):\n",
    "            def block(x,n_output, upscale=True):\n",
    "                h = tf.layers.batch_normalization(x)\n",
    "                h = tf.nn.relu(h)\n",
    "                h=tf.layers.conv2d(inputs=h,filters=n_output,strides=1,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu)\n",
    "                # second pre-activation\n",
    "                h = tf.layers.batch_normalization(h)\n",
    "                h = tf.nn.relu(h)\n",
    "                h = tf.layers.conv2d(h, filters=n_output, strides=1,kernel_size=[3,3], padding='same')\n",
    "                if upscale:\n",
    "                    # 1x1 conv2d\n",
    "                    f =tf.layers.conv2d(x,kernel_size=1, filters=n_output, strides=1, padding='same')\n",
    "                else:\n",
    "                    # identity\n",
    "                    f = x\n",
    "                # F_l(x) = f(x) + H_l(x):\n",
    "                t=tf.math.add(f, h)\n",
    "                return t\n",
    "            Input = tf.placeholder(dtype=tf.float32, shape=[None, 32, params['width_thresh'], 3], name='Input')\n",
    "            Seq_Len = tf.placeholder(dtype=tf.int32, shape=[None], name='Seq_len')\n",
    "            # input_layer = tf.placeholder(dtype=tf.float32, shape=[None, 32, params['width_thresh'], 3], name='Input')\n",
    "            conv1=tf.layers.conv2d(Input,filters=64,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu)\n",
    "\n",
    "            conv2=tf.layers.conv2d(conv1,filters=128,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu)\n",
    "\n",
    "            conv_2=tf.layers.conv2d(conv2,filters=256,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu)\n",
    "\n",
    "            pool1=tf.layers.max_pooling2d(inputs=conv_2,pool_size=[2,2],strides=[2,2],name=\"pool1\")\n",
    "\n",
    "            res_1=block(pool1, 256, upscale=False)\n",
    "\n",
    "            conv3=tf.layers.conv2d(res_1,filters=256,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu)\n",
    "            pool2=tf.layers.max_pooling2d(conv3,pool_size=[2,2],strides=2,name=\"pool2\")\n",
    "            res_2=block(pool2, 256, upscale=False)\n",
    "            res_3=block(res_2, 256, upscale=False)\n",
    "            conv4=tf.layers.conv2d(res_3,filters=256,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu)\n",
    "            pool3=tf.layers.max_pooling2d(conv4,pool_size=[1,2],strides=[1,2],name=\"pool3\")\n",
    "            res_4=block(pool3, 256, upscale=False)\n",
    "            res_5=block(res_4, 256, upscale=False)\n",
    "            res_6=block(res_5, 256, upscale=False)\n",
    "            res_7=block(res_6, 256, upscale=False)\n",
    "            res_8=block(res_7, 256, upscale=False)\n",
    "            conv_5=tf.layers.conv2d(res_8,filters=512,kernel_size=[3,3],padding=\"same\",activation=\"relu\")\n",
    "            pool_4=tf.layers.max_pooling2d(inputs=conv_5,pool_size=[2,2],strides=[2,2],name=\"pool4\")\n",
    "            res_9=block(pool_4,512,upscale=False)\n",
    "            res_10=block(res_9,512,upscale=False)\n",
    "            res_11=block(res_10,512,upscale=False)\n",
    "            conv_6=tf.layers.conv2d(res_11,filters=512,kernel_size=[3,3],padding=\"same\",activation=\"relu\")\n",
    "            pool_6=tf.layers.max_pooling2d(inputs=conv_6,pool_size=[2,2],strides=[2,2],name=\"pool6\")\n",
    "            pool_7=tf.layers.max_pooling2d(inputs=pool_6,pool_size=[2,2],strides=[2,2],name=\"pool7\")\n",
    "            conv_reshaped = tf.squeeze(pool_7, axis = [1], name='reshaped')\n",
    "\n",
    "\n",
    "            #b_norm6 = tf.nn.relu(tf.layers.batch_normalization(conv6, name='batch-norm3'),name='relu8')\n",
    "            #conv = tf.squeeze(b_norm6, axis = [2], name='reshaped')\n",
    "           # conv_reshaped = tf.squeeze(bnorm_6, axis = [1], name='reshaped')\n",
    "            #list_n_hidden = [256, 256]\n",
    "    num_units = 256\n",
    "    num_layers = 3\n",
    "    list_n_hidden = [256, 256]\n",
    "    with tf.name_scope('deep_bidirectional_lstm'):\n",
    "        fw_cell_list = [tf.contrib.rnn.BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "        bw_cell_list = [tf.contrib.rnn.BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "        lstm_net, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(fw_cell_list, bw_cell_list, conv_reshaped, sequence_length = Seq_Len,\n",
    "                                                                        dtype=tf.float32)\n",
    "        lstm_net = tf.nn.dropout(lstm_net, keep_prob=0.5)  # [width(time), batch, n_classes]\n",
    "        shape = lstm_net.get_shape().as_list()  # [batch, width, 2*n_hidden]\n",
    "        rnn_reshaped = tf.reshape(lstm_net, [-1, shape[-1]])  # [batch x width, 2*n_hidden]\n",
    "\n",
    "        W = tf.Variable(tf.truncated_normal(shape=[512, num_classes], mean=0.0, stddev=0.02))\n",
    "        b = tf.Variable(tf.constant(value=0.0, shape=[num_classes]))\n",
    "        fc_out = tf.nn.bias_add(tf.matmul(rnn_reshaped, W), b)\n",
    "        print(fc_out)\n",
    "\n",
    "        lstm_out = tf.reshape(fc_out, [-1, time_step, num_classes], name='reshape_out')\n",
    "        lstm_out = tf.transpose(lstm_out, [1, 0, 2], name='transpose_time_major')  # [width(time), batch, n_classes]\n",
    "        print(lstm_out)\n",
    "    return lstm_out\n",
    "\n",
    "def indexing(ids):   \n",
    "    CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "    import copy\n",
    "    vocab=set(ids)\n",
    "    vocab_to_int=copy.copy(CODES)\n",
    "    for vocab,integer in enumerate(vocab,len(CODES)):\n",
    "        vocab_to_int[integer] = vocab\n",
    "    int_to_vocab={ v_i: v for v, v_i in vocab_to_int.items()}\n",
    "    return vocab_to_int,int_to_vocab\n",
    "vocab_to_int,int_to_vocab=indexing(Y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compile():\n",
    "    \n",
    "    Target = tf.sparse_placeholder(tf.int32, name='Target')\n",
    "    \n",
    "\n",
    "    logits =ResNetCNN()\n",
    "    \n",
    "    decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits, Seq_Len, merge_repeated=False, beam_width=100, top_paths=2)\n",
    "\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), Target))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    table_init = tf.tables_initializer()\n",
    "    sess.run([init, table_init])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=52\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "#input_layer = tf.placeholder(dtype=tf.float32, shape=[None, 32, params['width_thresh'], 3], name='Input')\n",
    "#Outputs = tf.placeholder(tf.float32, shape = [BatchSize, 64, 2])\n",
    "#logits = tf.layers.dense(dec_outputs, 2, name  = 'logits') \n",
    "#Architecture(Input, outputs)\n",
    "\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "global sess\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config = config)\n",
    "graph = tf.get_default_graph()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"deep_bidirectional_lstm/BiasAdd:0\", shape=(?, 53), dtype=float32)\n",
      "Tensor(\"deep_bidirectional_lstm/transpose_time_major:0\", shape=(124, ?, 53), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Seq_len:0\", shape=(?,), dtype=int32) must be from the same graph as Tensor(\"deep_bidirectional_lstm/transpose_time_major:0\", shape=(124, ?, 53), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a6f881f35550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mCompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-98ae560c736c>\u001b[0m in \u001b[0;36mCompile\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mResNetCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctc_beam_search_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq_Len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_repeated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medit_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/ctc_ops.py\u001b[0m in \u001b[0;36mctc_beam_search_decoder\u001b[0;34m(inputs, sequence_length, beam_width, top_paths, merge_repeated)\u001b[0m\n\u001b[1;32m    296\u001b[0m       gen_ctc_ops.ctc_beam_search_decoder(\n\u001b[1;32m    297\u001b[0m           \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m           merge_repeated=merge_repeated))\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   return (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_ctc_ops.py\u001b[0m in \u001b[0;36mctc_beam_search_decoder\u001b[0;34m(inputs, sequence_length, beam_width, top_paths, merge_repeated, name)\u001b[0m\n\u001b[1;32m     97\u001b[0m                                 \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                                 \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                                 merge_repeated=merge_repeated, name=name)\n\u001b[0m\u001b[1;32m    100\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   5711\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5712\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5713\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5714\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5715\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   5647\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5648\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[0;32m-> 5649\u001b[0;31m                                                                 original_item))\n\u001b[0m\u001b[1;32m   5650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Seq_len:0\", shape=(?,), dtype=int32) must be from the same graph as Tensor(\"deep_bidirectional_lstm/transpose_time_major:0\", shape=(124, ?, 53), dtype=float32)."
     ]
    }
   ],
   "source": [
    "num_classes = params['num_classes']\n",
    "time_step = int(np.floor(params['width_thresh'] / 2) - 1)\n",
    "Input = tf.placeholder(dtype=tf.float32, shape=[None, 32, params['width_thresh'], 3], name='Input')\n",
    "Seq_Len = tf.placeholder(dtype=tf.int32, shape=[None], name='Seq_len')\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "global sess\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config = config)\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "Compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
